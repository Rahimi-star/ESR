{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame,Series\n",
    "import imputation\n",
    "import cross_validate as scv\n",
    "import log_worker as slog\n",
    "import plotting as splt\n",
    "from model_analsyis import nested_cross_validation_analysis as ncv_analysis\n",
    "import evaluation as evaluate\n",
    "import mathx\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************** GLOBAL PARAMETERS ******************************************\n",
    "data_dir = os.path.join(os.getcwd(),'data')\n",
    "input_data_file = os.path.abspath('temp__mc80_imputed_normalized_2019-08-23.csv')\n",
    "output_dir = os.path.join(data_dir, 'interim')\n",
    "figure_dir = output_dir\n",
    "metrics_output_file = os.path.join(output_dir,'scoring_metrics.txt')\n",
    "store_prediction_probs = True\n",
    "prediction_prob_dir = os.path.join(output_dir, 'prediction_probabilities')\n",
    "selected_features_file = os.path.join(output_dir,'selected_features.txt')\n",
    "seed = 287462\n",
    "num_folds = 10  # number of stratified folds for outside loop\n",
    "shuffle_on_split = False  # shuffle data before splitting into folds\n",
    "# number of features for feature selection, recommended number positive examples in training/ 10\n",
    "# if set to -1 use all features\n",
    "num_features = -1\n",
    "date_string='2019-08-23'\n",
    "file_prefix='temp_'\n",
    "\n",
    "slog.log_items(\"Global Parameters\\n\", metrics_output_file,\n",
    "               num_folds=num_folds,\n",
    "               num_features=num_features,\n",
    "               shuffle_on_split=shuffle_on_split,\n",
    "               seed=seed)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ************************* GLOBAL PARAMETERS END *************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "\n",
    "cases= pd.read_csv(os.path.join(os.path.join(data_dir,'raw'),'CASE_FILE.csv'))\n",
    "\n",
    "controls = pd.read_csv(os.path.join(os.path.join(data_dir,'raw'),'CONTROL_FILE.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Section\n",
    "cols_to_drop = []\n",
    "\n",
    "# column names for features requiring normalization\n",
    "cols_to_norm = ['gest_age', 'age', 'wbc', 'hgb', 'it_ratio',\n",
    "       'capPH', 'bicarb', 'glucose', 'creatinine', 'platelet_count', 'hr',\n",
    "       'rr', 'temp', 'sbp', 'dbp', 'map', 'weight', 'fio2',\n",
    "       'hr_delta', 'rr_delta', 'mabp_delta',\n",
    "       'temp_delta']\n",
    "# Sepsis Groups\n",
    "# Group 1: Culture Positive sepsis: culture positive, minimum 5 days antibiotic treatment\n",
    "# Group 2: Negative Sepsis: Culture negative, <72 hours antibiotic treatment\n",
    "# Group 3: Clinical Sepsis: Culture negative, >120 hours of antibiotic treatment\n",
    "# sepsis groups to use in positive samples (cases):\n",
    "CASE_GRPS = [1,3]\n",
    "\n",
    "# control group data is from non-septic periods for same individuals in case groups\n",
    "# if using controls file with unspecified groups, set to None\n",
    "CONTROL_GRPS = None\n",
    "missing_cutoff = 80\n",
    "\n",
    "cases['sepsis'] = 1\n",
    "controls['sepsis'] = 0\n",
    "\n",
    "# select data from specified sepsis groups\n",
    "if CONTROL_GRPS is None:\n",
    "    controls['sepsis_group'] = -1\n",
    "    CONTROL_GRPS = [-1]\n",
    "X = pd.concat([cases[cases['sepsis_group'].isin(CASE_GRPS)],\n",
    "               controls[controls['sepsis_group'].isin(CONTROL_GRPS)]],sort=True)\n",
    "\n",
    "of = os.path.join(data_dir ,'case_control_stats.txt')\n",
    "slog.log_line(\"samples count: {0}\\n\".format(len(X)), of)\n",
    "y = X['sepsis']\n",
    "slog.log_line('target counts\\n{0}\\n'.format(y.value_counts()), of)\n",
    "slog.log_line('Incidence Rate (percent): {0:.3f}\\n'.format(100 * np.sum(y) / float(len(y))), of)\n",
    "\n",
    "# Imputation\n",
    "missing_percentages = imputation.missing_percents(X)\n",
    "slog.log_dictionary(missing_percentages, 'Missing Data Percentages\\n', False,\n",
    "                          os.path.join(data_dir ,'missing_data_percents.txt'))\n",
    "\n",
    "# drop columns with missing percentage over threshold\n",
    "#cols_to_drop = []\n",
    "for k,v in missing_percentages.items():\n",
    "    if v > missing_cutoff:\n",
    "        cols_to_drop.append(k)\n",
    "\n",
    "for c in cols_to_drop:\n",
    "    X = X.drop(c,axis=1)\n",
    "    if c in cols_to_norm:\n",
    "        cols_to_norm.remove(c)\n",
    "        \n",
    "        # impute missing values\n",
    "imp = Imputer(strategy='mean')\n",
    "Ximp = imp.fit_transform(X)\n",
    "cnames = X.columns\n",
    "X = pd.DataFrame(data=Ximp, columns=cnames)\n",
    "\n",
    "# Normalization\n",
    "X[cols_to_norm] = X[cols_to_norm].apply(lambda x: (x-x.mean())/x.std())\n",
    "\n",
    "## Store processed data\n",
    "X.to_csv('{0}_mc{1}_imputed_normalized_{2}.csv'.\n",
    "                      format(file_prefix,missing_cutoff,date_string), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Trian and validate MODELS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run 0 of 10 for LogisticRegression\n",
      "\tStarting grid search ....\n",
      "\tGrid search complete ....\n",
      "Starting run 1 of 10 for LogisticRegression\n",
      "\tStarting grid search ....\n",
      "\tGrid search complete ....\n",
      "Starting run 2 of 10 for LogisticRegression\n",
      "\tStarting grid search ....\n",
      "\tGrid search complete ....\n",
      "Starting run 3 of 10 for LogisticRegression\n",
      "\tStarting grid search ....\n",
      "\tGrid search complete ....\n",
      "Starting run 4 of 10 for LogisticRegression\n",
      "\tStarting grid search ....\n",
      "\tGrid search complete ....\n",
      "Starting run 5 of 10 for LogisticRegression\n",
      "\tStarting grid search ....\n",
      "\tGrid search complete ....\n",
      "Starting run 6 of 10 for LogisticRegression\n",
      "\tStarting grid search ....\n",
      "\tGrid search complete ....\n",
      "Starting run 7 of 10 for LogisticRegression\n",
      "\tStarting grid search ....\n",
      "\tGrid search complete ....\n",
      "Starting run 8 of 10 for LogisticRegression\n",
      "\tStarting grid search ....\n",
      "\tGrid search complete ....\n",
      "Starting run 9 of 10 for LogisticRegression\n",
      "\tStarting grid search ....\n",
      "\tGrid search complete ....\n",
      "Logistic Regression\n",
      "accuracy:\tmean=0.780\tstd=0.030\trange=[0.707,0.818]\n",
      "f1:\tmean=0.640\tstd=0.040\trange=[0.566,0.703]\n",
      "sensitivity:\tmean=0.768\tstd=0.049\trange=[0.703,0.868]\n",
      "specificity:\tmean=0.784\tstd=0.035\trange=[0.691,0.809]\n",
      "precision:\tmean=0.549\tstd=0.041\trange=[0.452,0.604]\n",
      "ROC-AUC:\tmean=0.854\tstd=0.026\trange=[0.802,0.902]\n",
      "avg_precision:\tmean=0.707\tstd=0.047\trange=[0.585,0.760]\n",
      "npv:\tmean=0.908\tstd=0.018\trange=[0.889,0.946]\n"
     ]
    }
   ],
   "source": [
    "# import preprocessd data\n",
    "all_data = pd.read_csv(input_data_file).sample(frac=1, random_state=seed)\n",
    "y = all_data['sepsis']\n",
    "X = all_data.drop('sepsis', axis=1).drop('sepsis_group', axis=1)\n",
    "\n",
    "# Use stratified K-fold to get data split indices\n",
    "skf = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=shuffle_on_split)\n",
    "folds = {}\n",
    "fold_idx = 0\n",
    "for train_split, test_split in skf.split(X,y):\n",
    "    folds[fold_idx] = test_split\n",
    "    fold_idx += 1\n",
    "\n",
    "# evaluate models\n",
    "kn = num_features\n",
    "if num_features == -1:\n",
    "    kn = len(X.columns)\n",
    "\n",
    "# wrapper method to enable passing a random state for scoring method of feature selector\n",
    "def seeded_mutual_info_classif(X, y):\n",
    "    return mutual_info_classif(X,y, random_state=seed)\n",
    "\n",
    "feature_selector = SelectKBest(seeded_mutual_info_classif, k=kn)\n",
    "\n",
    "if store_prediction_probs and not os.path.exists(prediction_prob_dir):\n",
    "    os.makedirs(prediction_prob_dir)\n",
    "\n",
    "# ************************* Model evaluations ****************************************\n",
    "# --------------------------   logistic regression --------------------------------------------\n",
    "parameter_candidates = [{'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}]\n",
    "\n",
    "model = LogisticRegression(penalty='l2',class_weight='balanced', random_state=seed)\n",
    "ncv_analysis(model, parameter_candidates, folds, X, y, feature_selector, \"Logistic Regression\", \"LR\",\n",
    "             figure_dir, metrics_output_file, store_prediction_probs,\n",
    "             os.path.join(prediction_prob_dir, \"LR_pred_probs.csv\"),\n",
    "             os.path.join(prediction_prob_dir, \"LR_targets.csv\"), seed=seed,\n",
    "             selected_features_file=selected_features_file,\n",
    "             feature_coefs_file=os.path.join(output_dir,\"LR_coefs.csv\")\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run 0 of 10 for SVC\n",
      "\tStarting grid search ....\n",
      "\tGrid search complete ....\n",
      "Starting run 1 of 10 for SVC\n",
      "\tStarting grid search ....\n",
      "\tGrid search complete ....\n",
      "Starting run 2 of 10 for SVC\n",
      "\tStarting grid search ....\n",
      "\tGrid search complete ....\n",
      "Starting run 3 of 10 for SVC\n",
      "\tStarting grid search ....\n",
      "\tGrid search complete ....\n",
      "Starting run 4 of 10 for SVC\n",
      "\tStarting grid search ....\n",
      "\tGrid search complete ....\n",
      "Starting run 5 of 10 for SVC\n",
      "\tStarting grid search ....\n",
      "\tGrid search complete ....\n",
      "Starting run 6 of 10 for SVC\n",
      "\tStarting grid search ....\n",
      "\tGrid search complete ....\n",
      "Starting run 7 of 10 for SVC\n",
      "\tStarting grid search ....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#--------------------------  support vector machine ----------------------------------------------\n",
    "parameter_candidates = [{'C': [0.01, 0.1, 1, 10, 100],\n",
    "                         'gamma': [0.01, 0.1, 1, 10, 100]}]\n",
    "model = SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=seed)\n",
    "ncv_analysis(model, parameter_candidates, folds, X, y, feature_selector, \"SVM (RBF)\", \"SVM-RBF\",\n",
    "             figure_dir, metrics_output_file, store_prediction_probs,\n",
    "             os.path.join(prediction_prob_dir, \"SVM_pred_probs.csv\"),\n",
    "             os.path.join(prediction_prob_dir, \"SVM_targets.csv\"), seed=seed, n_jobs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  --------------------------  Gaussian NB -------------------------------------------------------\n",
    "model = GaussianNB()\n",
    "metric_values, fpr_scores, tpr_scores, precision_scores, recall_scores = \\\n",
    "   scv.nested_cross_validate(model, None, folds, X, y, feature_selector=feature_selector)\n",
    "ncv_analysis(model, None, folds, X, y, feature_selector, \"Naive Bayes\", \"NaiveBayes\",\n",
    "             figure_dir, metrics_output_file, store_prediction_probs,\n",
    "             os.path.join(prediction_prob_dir, \"NaiveBayes_pred_probs.csv\"),\n",
    "             os.path.join(prediction_prob_dir, \"NaiveBayes_targets.csv\"), seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  --------------------------  Gaussian Process -------------------------------------------\n",
    "model = GaussianProcessClassifier(random_state = seed)\n",
    "metric_values, fpr_scores, tpr_scores, precision_scores, recall_scores = \\\n",
    "    scv.nested_cross_validate(model, None, folds, X, y, feature_selector=feature_selector)\n",
    "ncv_analysis(model, None, folds, X, y, feature_selector, \"Gaussian Process\", \"GaussianProcess\",\n",
    "             figure_dir, metrics_output_file, store_prediction_probs,\n",
    "             os.path.join(prediction_prob_dir, \"GaussianProcess_pred_probs.csv\"),\n",
    "             os.path.join(prediction_prob_dir, \"GaussianProcess_targets.csv\"), seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  --------------------------  Random Forest --------------------------------------------\n",
    "parameter_candidates = [{'n_estimators': [10, 50, 100, 200],\n",
    "                         'criterion': ['gini','entropy'],\n",
    "                         'max_depth': [2, 4, 6]}]\n",
    "model = RandomForestClassifier(random_state=seed, class_weight='balanced')\n",
    "ncv_analysis(model, parameter_candidates, folds, X, y, feature_selector, \"Random Forest\", \"RandomForest\",\n",
    "             figure_dir, metrics_output_file, store_prediction_probs,\n",
    "             os.path.join(prediction_prob_dir, \"RandomForest_pred_probs.csv\"),\n",
    "             os.path.join(prediction_prob_dir, \"RandomForest_targets.csv\"), seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  --------------------------  AdaBoost -------------------------------------------------\n",
    "parameter_candidates = [{'base_estimator': [DecisionTreeClassifier(),\n",
    "                                            LogisticRegression(class_weight='balanced', random_state=seed),\n",
    "                                            SVC(kernel='rbf', probability=True,class_weight='balanced', random_state=seed)],\n",
    "                         'n_estimators': [50, 100],\n",
    "                         'learning_rate': [1.0, 0.5, 0.1]}]\n",
    "model = AdaBoostClassifier(random_state=seed)\n",
    "ncv_analysis(model, parameter_candidates, folds, X, y, feature_selector, \"AdaBoost\", \"AdaBoost\",\n",
    "             figure_dir, metrics_output_file, store_prediction_probs,\n",
    "             os.path.join(prediction_prob_dir, \"AdaBoost_pred_probs.csv\"),\n",
    "             os.path.join(prediction_prob_dir, \"AdaBoost_targets.csv\"), seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  --------------------------  KNN Classifier ---------------------------------------\n",
    "parameter_candidates = [{'n_neighbors': [5, 10],\n",
    "                         'weights': ['uniform', 'distance']}]\n",
    "model = KNeighborsClassifier()\n",
    "ncv_analysis(model, parameter_candidates, folds, X, y, feature_selector, \"KNN\", \"KNN\",\n",
    "             figure_dir, metrics_output_file, store_prediction_probs,\n",
    "             os.path.join(prediction_prob_dir, \"KNN_pred_probs.csv\"),\n",
    "             os.path.join(prediction_prob_dir, \"KNN_targets.csv\"), seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------  Gradient Boosting -------------------------------------------\n",
    "# wrapper class to fix sklearn bug\n",
    "class init:\n",
    "    def __init__(self, est):\n",
    "        self.est = est\n",
    "    def predict(self, X):\n",
    "        return self.est.predict_proba(X)[:,1][:,np.newaxis]\n",
    "    def fit(self, X, y, *kwarg):\n",
    "        self.est.fit(X, y)\n",
    "\n",
    "\n",
    "model = GradientBoostingClassifier(random_state=seed)\n",
    "parameter_candidates = [{'n_estimators': [50, 100, 200],\n",
    "                         'max_depth': [3, 5, 10],\n",
    "                         # 'init': [None,\n",
    "                         #          init(DecisionTreeClassifier(class_weight='balanced')),\n",
    "                         #          init(LogisticRegression(class_weight='balanced', random_state=seed)),\n",
    "                         #          init(SVC(kernel='rbf', probability=True,class_weight='balanced'))]\n",
    "                         }\n",
    "                        ]\n",
    "ncv_analysis(model, parameter_candidates, folds, X, y, feature_selector, \"Gradient Boost\", \"GradBoost\",\n",
    "             figure_dir, metrics_output_file, store_prediction_probs,\n",
    "             os.path.join(prediction_prob_dir, \"GradBoost_pred_probs.csv\"),\n",
    "             os.path.join(prediction_prob_dir, \"GradBoost_targets.csv\"), seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Postprocessing\n",
    "   \n",
    "   \n",
    "  #  ************************** GLOBAL PARAMETERS ******************************************\n",
    "data_dir = os.path.join('..','data')\n",
    "input_data_dir = os.path.join(data_dir, \"results\",\n",
    "                              \"PATH/TO/PREDICTION/PROBABILITES/FILE\",\n",
    "                              \"prediction_probabilities\")\n",
    "\n",
    "\n",
    "_prob_file = os.path.join(input_data_dir,\"{0}_pred_probs.csv\")\n",
    "_targ_file = os.path.join(input_data_dir,\"{0}_targets.csv\")\n",
    "file_prefixes = ['AdaBoost', 'GradBoost','GaussianProcess', 'KNN', 'LR', 'NaiveBayes', 'RandomForest', 'SVM']\n",
    "target_metric_name = evaluate.SENSITIVITY\n",
    "target_metric_value = 0.8\n",
    "ci_level = 0.95\n",
    "metrics_output_file = os.path.join(data_dir, 'interim', 'scoring_metrics_fixed_{0}_{1}.csv'.\n",
    "                                   format(target_metric_name,target_metric_value))\n",
    "\n",
    "metrics_ranges_output_file = os.path.join(data_dir, 'interim', 'scoring_metrics_ranges_fixed_{0}_{1}.csv'.\n",
    "                                   format(target_metric_name,target_metric_value))\n",
    "# ************************* GLOBAL PARAMETERS END *************************************\n",
    "def loaddata(file):\n",
    "    with open(file,'r') as f:\n",
    "        all_data = []\n",
    "        for line in f.readlines():\n",
    "            data = [float(x) for x in line.split(\",\")]\n",
    "            all_data.append(data)\n",
    "        return all_data\n",
    "\n",
    "line = \"model,acc,acc_std,acc_cil,acc_cih,f1,f1_std,f1_cil,f1_cih,sensitivity,sensitivity_std,sensitivity_cil,sensitivity_cih,\" \\\n",
    "       \"specificity,specificity_std,specificity_cil,specificity_cih,precision,precision_std,precision_cil,precision_cih,\" \\\n",
    "       \"npv,npv_std,npv_cil,npv_cih\\n\"\n",
    "slog.log_line(line, metrics_output_file)\n",
    "\n",
    "range_line = \"model,acc,acc_low,acc_high,f1,f1_low,f1_high,sensitivity,sensitivity_low,sensitivity_high,\" \\\n",
    "       \"specificity,specificity_low,specificity_high,precision,precision_low,precision_high,\" \\\n",
    "       \"npv,npv_low,npv_high\\n\"\n",
    "slog.log_line(range_line, metrics_ranges_output_file)\n",
    "\n",
    "for fp in file_prefixes:\n",
    "    probs = loaddata(_prob_file.format(fp))\n",
    "    targs = loaddata(_targ_file.format(fp))\n",
    "    acc, f1, sen, spec, precis, npv = evaluate.compute_metrics(targs, probs, target_metric_value, target_metric_name)\n",
    "    scores = [acc, f1, sen, spec, precis, npv]\n",
    "    line = \"{0},\".format(fp)\n",
    "    range_line = \"{0},\".format(fp)\n",
    "    for score in scores:\n",
    "        m, s, cil, cih = mathx.mean_confidence_interval(score, ci_level)\n",
    "        line=\"{0}{1},{2},{3},{4},\".format(line, m,s,cil,cih)\n",
    "        low = np.min(score)\n",
    "        high = np.max(score)\n",
    "        range_line=\"{0}{1},{2},{3},\".format(range_line, m, low, high)\n",
    "    line = \"{0}\\n\".format(line[0:-1])\n",
    "    range_line=\"{0}\\n\".format(range_line[0:-1])\n",
    "    slog.log_line(line, metrics_output_file)\n",
    "    slog.log_line(range_line, metrics_ranges_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
